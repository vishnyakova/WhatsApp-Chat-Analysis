{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook includes code that I will use to process the chat data. This code is refactored from original implementation in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji #for emojis processing \n",
    "import regex\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1) #column options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what is in current directory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data_location = \"WhatsApp Chat with LZ.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chat_data_location, encoding = 'utf8') as myfile:\n",
    "    head = [next(myfile) for x in range(5)]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(chat_data_location, sep = \"\\n\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis_list(text):\n",
    "# This function takes in text and returns a list of emojis. \n",
    "# Need emoji library for this\n",
    "# Borrowed from here: https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text/50530149#50530149 \n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize_udf(df, dtdiff):\n",
    "    # Helper function to sessionize data. \n",
    "    # takes a dataframe and date differences in minutes \n",
    "    # Function can be used to make sessions that are not spearated by a period longer than X minutes.\n",
    "    # Returns a list that can be assigned to a new column\n",
    "    df = df.sort_values(by = \"id\") #sort \n",
    "    df[\"event_dt_lagged\"] = df[\"event_dt\"].shift(1) #get lagged time \n",
    "    df[\"diff\"] = (df.event_dt - df.event_dt_lagged).astype(\"timedelta64[m]\") #calculate minute difference \n",
    "    df[\"session_id\"] = df[\"diff\"].apply(lambda x: 1 if (x > dtdiff) | (np.isnan(x)) else 0) # make a flag for new session start\n",
    "    return df.session_id.cumsum() #get session id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = data.message.str.extract(r\"(^\\d*\\/\\d*\\/\\d{2}, \\d*:\\d*)?(?: - )?(.+?: )?(.*)\") #get 3 groupos: event_dt, author, text\n",
    "data_e.columns = [\"event_dt\", \"author\", \"text\"] #rename columns \n",
    "data_e.author = data_e.author.str.replace(\":\", \"\") #get rid of : in author name\n",
    "data_e = data_e.fillna(method = 'ffill') #fill missing values forward\n",
    "data_e = data_e.dropna() #drop na - this will get rid of first message without author from WhatsApp\n",
    "data_e[\"text\"] = data_e.groupby([\"event_dt\", \"author\"])[\"text\"].transform(' '.join) #merge rows in the same minute\n",
    "data_e = data_e.drop_duplicates([\"event_dt\", \"author\",\"text\"]) #drop dups\n",
    "data_e[\"event_dt\"] = pd.to_datetime(data_e.event_dt, format = \"%m/%d/%y, %H:%M\" ) #convert to date\n",
    "data_e[\"media_flag\"] = data_e.text.str.contains(\"<Media omitted>\")\n",
    "data_e[\"link_flag\"] = data_e.text.str.contains(\"http://|https://\")\n",
    "data_e[\"id\"] = np.arange(len(data_e)) #make row id\n",
    "data_e[\"emojis\"] = data_e.text.apply(extract_emojis_list)\n",
    "data_e[\"emoji_description\"] = data_e.emojis.apply(lambda emojis_list: [emoji.demojize(x) for x in emojis_list])\n",
    "data_e[\"date\"] = data_e.event_dt.dt.date\n",
    "data_e[\"hour\"] = data_e.event_dt.dt.hour\n",
    "data_e[\"weekday\"] = data_e.event_dt.dt.weekday_name\n",
    "data_e[\"session_id60\"] = sessionize_udf(data_e, 60) #sessions without a break more than 1 hour\n",
    "data_e[\"session_id180\"] = sessionize_udf(data_e, 60*3) #session without a break more than 3 hours\n",
    "data_e[\"message_lag\"] = (data_e.event_dt - data_e.event_dt.shift(1)).astype(\"timedelta64[m]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e.to_csv(\"data_e.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
